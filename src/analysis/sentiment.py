from transformers import AutoModelForSequenceClassification, AutoTokenizer 
import torch
import torch.nn as nn
import json
from typing import List, Dict 
import matplotlib.pyplot as plt
import numpy as np

class LSTMSmoother(nn.Module):
    """LSTM –º–æ–¥–µ–ª—å –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self, hidden_size: int = 32, num_layers: int = 2, dropout: float = 0.2):
        super(LSTMSmoother, self).__init__()
        self.lstm = nn.LSTM(
            input_size=1,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout if num_layers > 1 else 0
        )
        self.fc = nn.Linear(hidden_size * 2, 1)
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # x shape: (batch, seq_len, 1)
        lstm_out, _ = self.lstm(x)
        lstm_out = self.dropout(lstm_out)
        # lstm_out shape: (batch, seq_len, hidden_size * 2)
        out = self.fc(lstm_out)
        return out.squeeze(-1)


class SentimentAnalyzer:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –∫–ª–∞—Å—Å: –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ + LSTM –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è"""
    
    def __init__(self, model_name: str = 'Tochka-AI/ruRoPEBert-classic-base-2k'):
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏...")
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self.model_name = model_name
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name, 
            trust_remote_code=True, 
            attn_implementation='eager'
        )
        
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.model = self.model.to(self.device)
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # LSTM –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
        self.lstm_smoother = None

    def predict_sentiment(self, texts: List[str], batch_size: int = 8) -> List[float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ (—Å –±–∞—Ç—á–∏–Ω–≥–æ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        self.model.eval()
        sentiment_out = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            
            with torch.no_grad():
                inputs = self.tokenizer(
                    batch_texts, 
                    return_tensors='pt', 
                    truncation=True, 
                    padding=True,
                    max_length=512
                ).to(self.device)
                
                logits = self.model(**inputs).logits
                proba = torch.sigmoid(logits).cpu().numpy()
                
                sentiment_out.extend([float(p[0]) for p in proba])
        
        return sentiment_out
    
    def train_lstm_smoother(self, data: List[float], hidden_size: int = 32, 
                           num_layers: int = 2, epochs: int = 100, 
                           lr: float = 0.01, patience: int = 10) -> LSTMSmoother:
        """
        –û–±—É—á–∞–µ—Ç LSTM –º–æ–¥–µ–ª—å –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞
        
        Args:
            data: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            hidden_size: —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è LSTM
            num_layers: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ LSTM
            epochs: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
            lr: learning rate
            patience: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è early stopping
        """
        if len(data) < 10:
            return None
        
        print(f"\n–û–±—É—á–µ–Ω–∏–µ LSTM —Å–≥–ª–∞–∂–∏–≤–∞—Ç–µ–ª—è...")
        print(f"–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {num_layers} —Å–ª–æ—è, hidden_size={hidden_size}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        model = LSTMSmoother(hidden_size, num_layers).to(self.device)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        data_tensor = torch.FloatTensor(data).unsqueeze(0).unsqueeze(-1).to(self.device)
        target = data_tensor.clone()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å weight decay –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏
        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
        criterion = nn.MSELoss()
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5
        )
        
        # Early stopping
        best_loss = float('inf')
        patience_counter = 0
        
        model.train()
        for epoch in range(epochs):
            optimizer.zero_grad()
            output = model(data_tensor)
            loss = criterion(output, target.squeeze(-1))
            loss.backward()
            
            # Gradient clipping –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            scheduler.step(loss)
            
            # Early stopping
            if loss.item() < best_loss:
                best_loss = loss.item()
                patience_counter = 0
            else:
                patience_counter += 1
            
            if (epoch + 1) % 20 == 0:
                print(f"–≠–ø–æ—Ö–∞ {epoch + 1}/{epochs}, Loss: {loss.item():.6f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
            
            if patience_counter >= patience:
                print(f"Early stopping –Ω–∞ —ç–ø–æ—Ö–µ {epoch + 1}")
                break
        
        print(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –§–∏–Ω–∞–ª—å–Ω–∞—è loss: {best_loss:.6f}")
        return model
    
    def apply_lstm_smoothing(self, data: List[float], lstm_model: LSTMSmoother) -> List[float]:
        """–ü—Ä–∏–º–µ–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é LSTM –º–æ–¥–µ–ª—å –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è"""
        if lstm_model is None or len(data) < 10:
            return data
        
        lstm_model.eval()
        data_tensor = torch.FloatTensor(data).unsqueeze(0).unsqueeze(-1).to(self.device)
        
        with torch.no_grad():
            smoothed = lstm_model(data_tensor)
            smoothed = smoothed.cpu().numpy()[0]
        
        return smoothed.tolist() 
    
class YouTubeSentimentAnalyzer(SentimentAnalyzer):
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ YouTube –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤"""
    
    def __init__(self, model_name: str = 'Tochka-AI/ruRoPEBert-classic-base-2k'):
        super().__init__(model_name)

    def load_comments_from_json(self, json_file: str) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data.get('comments', [])

    def analyze_from_json(self, json_file: str, 
                         output_plot: str = 'sentiment_analysis.png',
                         use_lstm_smoothing: bool = True,
                         lstm_hidden_size: int = 64,
                         lstm_layers: int = 2,
                         lstm_epochs: int = 100,
                         batch_size: int = 8) -> Dict:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ JSON
        
        Args:
            json_file: –ø—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏
            output_plot: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞
            use_lstm_smoothing: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å LSTM —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
            lstm_hidden_size: —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è LSTM
            lstm_layers: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ LSTM
            lstm_epochs: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è LSTM
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
            
        Returns:
            Dict: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        print(f"\n{'='*60}")
        print(f"–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤")
        print(f"{'='*60}\n")
        
        print(f"–ó–∞–≥—Ä—É–∂–∞—é –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–∑ {json_file}...")
        comments = self.load_comments_from_json(json_file)
        
        if not comments:
            print("‚ùå –ù–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return {}
        
        print(f"‚úì –ù–∞–π–¥–µ–Ω–æ {len(comments)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        texts = [c.get('text', '') for c in comments if c.get('text')]
        
        if not texts:
            print("‚ùå –ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return {}
        
        print(f"\nüìä –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ {len(texts)} –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤...")
        print(f"–ú–æ–¥–µ–ª—å: {self.model_name}")
        print(f"Batch size: {batch_size}")
        
        sentiments = self.predict_sentiment(texts, batch_size=batch_size)
        print(f"‚úì –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º LSTM —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ
        filtered_sentiments = sentiments
        if use_lstm_smoothing and len(sentiments) > 10:
            lstm_model = self.train_lstm_smoother(
                sentiments, 
                hidden_size=lstm_hidden_size,
                num_layers=lstm_layers,
                epochs=lstm_epochs
            )
            filtered_sentiments = self.apply_lstm_smoothing(sentiments, lstm_model)
            print(f"‚úì LSTM —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ")
        
        # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
        self._create_plot(sentiments, filtered_sentiments, output_plot, use_lstm_smoothing)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        results = self._calculate_statistics(comments, sentiments, filtered_sentiments, output_plot)
        self._print_statistics(results)
        
        return results
    
    def _create_plot(self, sentiments: List[float], filtered: List[float], 
                    output_plot: str, use_smoothing: bool):
        """–°–æ–∑–¥–∞–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –∞–Ω–∞–ª–∏–∑–∞"""
        plt.figure(figsize=(14, 7), dpi=300)
        
        # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
        plt.subplot(2, 1, 1)
        plt.plot(sentiments, alpha=0.4, label='–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ', color='#3498db', linewidth=1)
        
        if use_smoothing:
            plt.plot(filtered, linewidth=2.5, label='LSTM —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ', color='#e74c3c')
        
        plt.xlabel('–ù–æ–º–µ—Ä –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è', fontsize=11)
        plt.ylabel('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', fontsize=11)
        plt.title(f'–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ (–≤—Å–µ–≥–æ: {len(sentiments)})', 
                 fontsize=13, fontweight='bold')
        plt.legend(fontsize=10)
        plt.grid(alpha=0.3, linestyle='--')
        
        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        plt.subplot(2, 1, 2)
        plt.hist(sentiments, bins=50, alpha=0.7, color='#3498db', edgecolor='black')
        plt.xlabel('–¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å', fontsize=11)
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤', fontsize=11)
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏', fontsize=12, fontweight='bold')
        plt.axvline(np.mean(sentiments), color='#e74c3c', linestyle='--', 
                   linewidth=2, label=f'–°—Ä–µ–¥–Ω–µ–µ: {np.mean(sentiments):.3f}')
        plt.legend(fontsize=10)
        plt.grid(alpha=0.3, linestyle='--', axis='y')
        
        plt.tight_layout()
        plt.savefig(output_plot)
        plt.close()
        
        print(f"‚úì –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_plot}")
    
    def _calculate_statistics(self, comments: List[Dict], sentiments: List[float],
                             filtered: List[float], output_plot: str) -> Dict:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∞–Ω–∞–ª–∏–∑–∞"""
        positive = sum(1 for s in sentiments if s > 0.6)
        negative = sum(1 for s in sentiments if s < 0.4)
        neutral = len(sentiments) - positive - negative
        
        return {
            "total_comments": len(comments),
            "analyzed_comments": len(sentiments),
            "average_sentiment": float(np.mean(sentiments)),
            "median_sentiment": float(np.median(sentiments)),
            "std_sentiment": float(np.std(sentiments)),
            "min_sentiment": float(np.min(sentiments)),
            "max_sentiment": float(np.max(sentiments)),
            "positive_comments": positive,
            "negative_comments": negative,
            "neutral_comments": neutral,
            "positive_percentage": (positive / len(sentiments)) * 100,
            "negative_percentage": (negative / len(sentiments)) * 100,
            "neutral_percentage": (neutral / len(sentiments)) * 100,
            "smoothed_average": float(np.mean(filtered)),
            "output_plot": output_plot
        }
    
    def _print_statistics(self, results: Dict):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≤ –∫–æ–Ω—Å–æ–ª—å"""
        print(f"\n{'='*60}")
        print(f"üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê")
        print(f"{'='*60}")
        print(f"–í—Å–µ–≥–æ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤: {results['analyzed_comments']}")
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:")
        print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ:  {results['average_sentiment']:.3f}")
        print(f"  ‚Ä¢ –ú–µ–¥–∏–∞–Ω–∞:  {results['median_sentiment']:.3f}")
        print(f"  ‚Ä¢ –°—Ç. –æ—Ç–∫–ª: {results['std_sentiment']:.3f}")
        print(f"  ‚Ä¢ –ú–∏–Ω:      {results['min_sentiment']:.3f}")
        print(f"  ‚Ä¢ –ú–∞–∫—Å:     {results['max_sentiment']:.3f}")
        print(f"\nüòä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        print(f"  ‚Ä¢ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ (>0.6): {results['positive_comments']} ({results['positive_percentage']:.1f}%)")
        print(f"  ‚Ä¢ –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ:       {results['neutral_comments']} ({results['neutral_percentage']:.1f}%)")
        print(f"  ‚Ä¢ –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ (<0.4): {results['negative_comments']} ({results['negative_percentage']:.1f}%)")
        print(f"\nüéØ –°–≥–ª–∞–∂–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ: {results['smoothed_average']:.3f}")
        print(f"{'='*60}\n")
